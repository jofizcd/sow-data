{
  "version": "2.2.1",
  "last_updated": "2026-02-08",
  "models": [
    {
      "hf_id": "CrucibleLab/L3.3-70B-Loki-V2.0-GGUF",
      "name": "L3.3-70B-Loki-V2.0",
      "author": "CrucibleLab",
      "downloads": 1500,
      "min_ram_gb": 32,
      "recommended_ram_gb": 64,
      "min_vram_gb": 0,
      "recommended_vram_gb": 40,
      "optimal_quant": "Q4_K_M",
      "description_en": "L3.3-70B-Loki-V2.0 (aka Loki-v2-70B) is a narrative/DM-focused fine-tune of Llama-3.3-70B-Instruct, trained on a massive custom 600M+ token dataset specifically designed for deeply immersive roleplay, TTRPG-style storytelling, and long-form narrative experiences. It excels at acting as a Dungeon Master: controlling worlds with real consequences, stakes, consistent lore adherence, expressive prose, clear narrative roles, and handling complex scenarios (QA, prose, ERP, dark themes). Users praise its strong consistency over long sessions, vivid descriptions, emotional depth, unexpected plot developments, and ability to maintain character/world integrity without degenerating into generic chatbot responses. It supports both SFW epic adventures and NSFW/dark content without heavy censorship or quality drops. The model card includes detailed guides on prompting, worlds/universes, and presets. Early 2026 feedback positions it as one of the top 70B RP models, often compared to Magnum or GLM variants but with stronger narrative focus. Requires significant hardware (40+ GB VRAM recommended for smooth Q4/Q5 speeds); use temperature 0.7–1.0 and follow the extensive usage guide in the model card.",
      "description_ru": "L3.3-70B-Loki-V2.0 (он же Loki-v2-70B) — это нарративный/DM-ориентированный файнтьюн Llama-3.3-70B-Instruct, обученный на огромном кастомном датасете 600M+ токенов специально для глубокого иммерсивного ролевого опыта, стиля TTRPG и длинных нарративных историй. Модель идеально играет роль Dungeon Master: управляет мирами с реальными последствиями, ставками, строгим соблюдением лора, выразительной прозой, чёткими нарративными ролями и сложными сценариями (QA, проза, ERP, тёмные темы). Пользователи хвалят высокую консистентность на длинных сессиях, яркие описания, эмоциональную глубину, неожиданные сюжетные повороты и способность держать персонажей/мир без деградации в 'чатбот-стиль'. Поддерживает как SFW-эпические приключения, так и NSFW/тёмный контент без цензуры или падений качества. Модельная карточка содержит подробные гайды по промптингу, мирам/вселенным и пресетам. По отзывам начала 2026 года — одна из топовых 70B RP-моделей, часто сравнивают с Magnum или GLM, но с большим акцентом на нарратив. Требует мощного железа (рекомендуется 40+ ГБ VRAM для комфортных Q4/Q5); используйте температуру 0.7–1.0 и обязательно читайте гайд в карточке модели.",
      "author_notes": "Dataset breakdown: QA Lines 46,800+, Prose Lines 19,800+, ERP Lines 16,600+, Dark Lines 12,500+. Format: Llama-3 Instruct. Extensive model card guide for prompting, world-building. GGUF/EXL3 quants available (e.g., from CrucibleLab-TG or community). Temperature 0.7–1.0 recommended; avoid overly restrictive presets to preserve narrative freedom."
    },
    {
      "hf_id": "TheDrummer/Valkyrie-49B-v2.1-GGUF",
      "name": "Valkyrie-49B-v2.1",
      "author": "TheDrummer",
      "downloads": 120,
      "min_ram_gb": 32,
      "recommended_ram_gb": 64,
      "min_vram_gb": 0,
      "recommended_vram_gb": 35,
      "optimal_quant": "Q5_K_M",
      "description_en": "Valkyrie-49B-v2.1 is the latest iteration in TheDrummer's Valkyrie series — a fine-tune of NVIDIA's Llama-3.3-Nemotron-Super-49B-v1.5, optimized for creative writing, immersive storytelling, and lifelike character interactions. This 50B-parameter model delivers high-quality prose, strong understanding of complex concepts, natural and expressive responses, excellent handling of multi-character scenarios, group dynamics, and detailed narratives. Users highlight its 'life-like' reactions, vivid writing style, good reasoning capabilities (close to higher-tier 70B models in many tasks), and minimal repetition or generic output. It performs well in both SFW creative scenarios and uncensored/NSFW content without heavy refusals or quality drops. Early 2026 feedback calls it a 'pack puncher' and strong compromise between size and performance — often praised as one of the best 49B-class models for detailed, engaging roleplay-style interactions. GGUF quants (Q4_K_S to Q6_K) run comfortably on mid-to-high-end hardware; use temperature around 0.7–1.0 for best creativity. While still gathering widespread tests (very recent release), it's already seen as a solid upgrade over v2 and a go-to for users wanting smarter, more coherent output without jumping to full 70B.",
      "description_ru": "Valkyrie-49B-v2.1 — это свежий апдейт в серии Valkyrie от TheDrummer: файнтьюн на базе NVIDIA Llama-3.3-Nemotron-Super-49B-v1.5, заточенный под креативное письмо, иммерсивный сторителлинг и живые взаимодействия персонажей. Эта 50B-модель выдаёт высококачественную прозу, отлично понимает сложные концепции, даёт естественные и выразительные ответы, хорошо справляется с мульти-персонажными сценариями, групповой динамикой и детализированными нарративами. Пользователи отмечают 'life-like' реакции, яркий стиль письма, сильные рассуждения (близко к 70B в многих задачах) и минимум повторений или шаблонности. Работает уверенно как в SFW-креативе, так и в uncensored/NSFW без сильных отказов или падений качества. По отзывам начала 2026 года — это 'pack puncher' и отличный компромисс по размеру/качеству, один из лучших 49B-классов для детализированных, увлекательных ролевых взаимодействий. GGUF-кванты (Q4_K_S до Q6_K) комфортно запускаются на среднем-высоком железе; используйте температуру 0.7–1.0 для оптимальной креативности. Модель очень новая, но уже считается солидным апгрейдом v2 и фаворитом для тех, кто хочет умную и качественную модель без перехода на полные 70B.",
      "author_notes": "Base: Llama-3.3-Nemotron-Super-49B-v1.5. Capable of strong reasoning and creative output. GGUF quants available (bartowski/TheDrummer_Valkyrie-49B-v2.1-GGUF recommended for imatrix). Temperature 0.7–1.0 suggested; excels at lifelike characters and scenarios."
    },
    {
      "hf_id": "TheDrummer/Cydonia-24B-v4.3-GGUF",
      "name": "Cydonia-24B-v4.3",
      "author": "TheDrummer",
      "downloads": 10760,
      "min_ram_gb": 12,
      "recommended_ram_gb": 16,
      "min_vram_gb": 0,
      "recommended_vram_gb": 16,
      "optimal_quant": "Q4_K_M",
      "description_en": "The Cydonia 24B v4.3 from TheDrummer is perhaps the most balanced local model for role-playing games in the 24B segment. Her main advantage is her amazing ability to take the initiative in the plot: the model independently introduces relevant elements that are not explicitly mentioned in the prompt, but fit seamlessly into the story. As noted by users who have used this model, Cydonia 24B v4.3 creates the feeling of a live interlocutor on the other side of the language model, rather than a simple passive performer of commands. The model does an excellent job with group chats - when tested with four characters (including the user), it retained the unique voice of each. The model works with and without thinking tokens, but in version v4.3, the developer intentionally mitigated the excessive aggressiveness of previous versions - now characters are less likely to show obsessive attachment without a direct hint from the user, which makes dialogues more natural. With a context of up to 32,000 tokens, the model retains the consistency of the character and remembers the details mentioned 5-10 thousand tokens ago. The only warning: in rare cases, minor errors in physics or cause-and-effect relationships are possible, but they can be easily fixed with a single swipe. For those who appreciate literary quality, this is a great example of a model writer whose answers rarely require editing.",
      "description_ru": "Cydonia 24B v4.3 от TheDrummer - это, пожалуй, самая сбалансированная локальная модель для ролевых игр в сегменте 24B. Её главное преимущество - удивительная способность брать инициативу в сюжете: модель самостоятельно вводит релевантные элементы, не упомянутые явно в промпте, но органично вписывающиеся в историю. Как отмечают пользователи, которые пользовались этой моделью, Cydonia 24B v4.3 создаёт ощущение живого собеседника по ту сторону языковой модели, а не простого пассивного исполнителя команд. Модель отлично справляется с групповыми чатами - при тестировании с четырьмя персонажами (включая пользователя) она сохраняла уникальный голос каждого. Модель работает как с токенами thinking, так и без них, но в версии v4.3 разработчик намеренно смягчил излишнюю «агрессивность» предыдущих версий - теперь персонажи реже проявляют навязчивую привязанность без прямого намёка от пользователя, что делает диалоги более естественными. При контексте до 32 тысяч токенов модель сохраняет консистентность персонажа и помнит детали, упомянутые 5-10 тысяч токенов назад. Единственное предупреждение: в редких случаях возможны незначительные ошибки в физике или причинно-следственных связях, но они легко исправляются одним свайпом. Для тех, кто ценит литературное качество - это отличный пример модели-писателя, чьи ответы редко требуют редактирования.",
      "author_notes": "-"
    },
    {
      "hf_id": "mradermacher/WeirdCompound-v1.7-24b-GGUF",
      "name": "WeirdCompound-v1.7-24b",
      "author": "FlareRebellion",
      "downloads": 113,
      "min_ram_gb": 12,
      "recommended_ram_gb": 16,
      "min_vram_gb": 0,
      "recommended_vram_gb": 16,
      "optimal_quant": "Q4_K_M",
      "description_en": "WeirdCompound v1.7-24b is a merge through the Model Stock method based on Cydonia v4.2.0 with the addition of Dans-PersonalityEngine-V1.3.0-24b, Eurydice-24b-v3.5 and other components, creating a unique hybrid character. The model demonstrates outstanding creativity and literary skill in SFW scenarios: users report stunning descriptions, deep dialogues, and unexpected plot twists. However, it is critically important to understand her key limitation - in NSFW scenes, the model loses that spark of creativity, giving out formulaic, predictable lines, whereas in ordinary dialogue her prose can compete with top 70B models. This makes WeirdCompound an ideal choice for literary role-playing games, fantasy adventures, or dramatic scenarios without explicit content, but completely unsuitable for erotic role-playing games. If you plan to use the model in mixed mode (both SFW and NSFW), be prepared for a sharp drop in quality when switching to intimate scenes - many users prefer to keep Cydonia as the main model, and use WeirdCompound selectively only for clean storylines.",
      "description_ru": "WeirdCompound v1.7-24b - это сложный мерж через метод Model Stock на базе Cydonia v4.2.0 с добавлением Dans-PersonalityEngine-V1.3.0-24b, Eurydice-24b-v3.5 и других компонентов, создающий уникальный гибридный характер. Модель демонстрирует выдающуюся креативность и литературное мастерство в SFW-сценариях: пользователи сообщают о «потрясающих описаниях, глубоких диалогах и неожиданных поворотах сюжета». Однако критически важно понимать её ключевое ограничение - в NSFW сценах модель теряет ту самую искру креативности, выдавая шаблонные, предсказуемые реплики, тогда как в обычном диалоге её проза может соперничать с топовыми 70B моделями. Это делает WeirdCompound идеальным выбором для литературных ролевых игр, фэнтези-приключений или драматических сценариев без откровенного контента, но совершенно неподходящим для эротических ролевых игр. Если вы планируете использовать модель в смешанном режиме (и SFW, и NSFW), будьте готовы к резкому падению качества при переходе в интимные сцены - многие пользователи предпочитают держать Cydonia как основную модель, а WeirdCompound использовать выборочно только для «чистых» сюжетных линий.",
      "author_notes": "-"
    },
    {
      "hf_id": "mradermacher/RP-Spectrum-24B-i1-GGUF",
      "name": "RP-Spectrum-24B",
      "author": "Casual-Autopsy",
      "downloads": 82,
      "min_ram_gb": 12,
      "recommended_ram_gb": 16,
      "min_vram_gb": 0,
      "recommended_vram_gb": 16,
      "optimal_quant": "Q4_K_M",
      "description_en": "RP-Spectrum-24B is a high-quality merge created using the DELLA method (arxiv:2406.11617) on top of Mistral-Small-3.2-24B-Instruct-2506, incorporating some of the community's strongest RP finetunes (including influences from Cydonia, Dark-Nexus, Magnum-Diamond and similar top RP models). The result is a very creative and versatile 24B model optimized specifically for roleplay. Users note excellent prose quality, strong adherence to character cards and lorebooks, vivid descriptions, emotional depth, and good handling of both SFW storytelling and NSFW/erotic scenes without heavy censorship or sudden quality drops. It excels in long immersive sessions with diverse scenarios (fantasy, drama, combat RP), delivering unexpected twists and consistent personalities. Recommended temperature ≤0.7 to avoid excessive randomness. While still a very new merge (early 2026), early feedback calls it 'quite good' and a strong contender among 24B RP models, often compared favorably to Maginum-Cydoms or Cydonia variants. Ideal for users seeking a balanced, creative RP experience on mid-range hardware (16–24 GB VRAM recommended for comfortable speeds).",
      "description_ru": "RP-Spectrum-24B — это высококачественный мерж, созданный методом DELLA (arxiv:2406.11617) на базе Mistral-Small-3.2-24B-Instruct-2506 с включением сильнейших RP-файнтьюнов сообщества (влияния Cydonia, Dark-Nexus, Magnum-Diamond и подобных топовых RP-моделей). Получилась очень креативная и универсальная 24B-модель, специально заточенная под ролевые игры. Пользователи отмечают отличное качество прозы, сильное следование карточкам персонажей и лорбуккам, яркие описания, эмоциональную глубину и хорошую работу как в SFW-сюжетах, так и в NSFW/эротических сценах без резких падений качества или цензуры. Модель хорошо справляется с длинными иммерсивными сессиями, разнообразными сценариями (фэнтези, драма, боевой RP), выдаёт неожиданные повороты и держит консистентность персонажей. Рекомендуется температура ≤0.7, чтобы избежать избыточной случайности. Модель очень новая (начало 2026), но первые отзывы называют её 'quite good' и сильным конкурентом среди 24B RP-моделей — часто сравнивают с Maginum-Cydoms или вариантами Cydonia. Идеальный выбор для тех, кто ищет сбалансированный, креативный RP на среднем железе (рекомендуется 16–24 ГБ VRAM для комфортной скорости).",
      "author_notes": "Temperature ≤0.7 recommended due to high creativity. Use with lorebooks for best character consistency. GGUF quants from mradermacher (Q4_K_S/M — fast and recommended)."
    },
    {
      "hf_id": "mradermacher/Maginum-Cydoms-24B-GGUF",
      "name": "Maginum-Cydoms-24B",
      "author": "Casual-Autopsy",
      "downloads": 185,
      "min_ram_gb": 12,
      "recommended_ram_gb": 16,
      "min_vram_gb": 0,
      "recommended_vram_gb": 16,
      "optimal_quant": "Q4_K_M",
      "description_en": "Maginum-Cydoms-24B is a multi-step merge (DELLA, TIES, SLERP) based on Mistral-Small / Magistral-Small architecture, heavily tuned for roleplay, creative writing, and NSFW scenarios. It's designed to deliver vivid, high-quality prose, strong narrative flow, and immersive interactions, often praised as 'absolute peak' or 'really good at writing' in RP contexts. The model handles dark themes, and detailed descriptions confidently without heavy censorship. However, community feedback highlights limitations: weaker adherence to character cards/personalities (e.g., makes shameless characters shameful or timid ones confident), positivity bias from base model, poor instruction/rules following, occasional logical failures, multi-language issues (inserts English words), and repetition in some setups. Many users fix this by putting personality details into advanced definitions for constant reminders. Early 2026 tests position it as a strong 24B contender for prose-heavy RP, but it requires tweaking (e.g., low temp, strong reminders in prompts) to shine. Runs comfortably on mid-range hardware (Q4_K_M ~16 GB VRAM); marked as sensitive/NSFW — use with caution for explicit content.",
      "description_ru": "Maginum-Cydoms-24B — это многоступенчатый мерж (DELLA, TIES, SLERP) на базе Mistral-Small / Magistral-Small, сильно заточенный под roleplay, креативное письмо, и NSFW-сценарии. Модель выдаёт яркую, высококачественную прозу, сильный нарративный поток и иммерсивные взаимодействия — часто называют 'абсолютным пиком' или 'действительно хорошим в написании текстов' в RP-контексте. Хорошо справляется с тёмными темами и детализированными описаниями без сильной цензуры. Однако отзывы сообщества отмечают минусы: слабое следование карточкам персонажей (делает бесстыдный — стыдливый, робкий — уверенный в себе), склонность к позитиву от базовой модели, проблемы со следованием правилам и инструкциям, логические сбои, вставки английских слов в другие языки и повторения. Многие фиксят это, перенося личность в расширенные определения для постоянного напоминания. По тестам начала 2026 — сильный 24B-кандидат для RP с тяжелой прозой, но требует тюнинга (низкая температура, постоянные напоминания в промптах). Комфортно на среднем железе (Q4_K_M ~16 ГБ VRAM); помечена как чувствительная/NSFW — используйте осторожно с NSFW контентом.",
      "author_notes": "Merge methods: DELLA, TIES, SLERP, multi-step. Base: Mistral-Small / Magistral-Small. GGUF quants from mradermacher (recommended: Q4_K_M or i1/imatrix for better quality). Variants: absolute-heresy (MuXodious). Temperature low (≤0.7) suggested to control creativity/positivity. Sensitive content — explicit, NSFW, ERP-focused. Strong prose but tune character adherence via advanced defs or prompts."
    },
    {
      "hf_id": "mradermacher/Snowpiercer-15B-v4-GGUF",
      "name": "Snowpiercer-15B-v4",
      "author": "TheDrummer",
      "downloads": 75,
      "min_ram_gb": 12,
      "recommended_ram_gb": 16,
      "min_vram_gb": 0,
      "recommended_vram_gb": 12,
      "optimal_quant": "Q4_K_M",
      "description_en": "Snowpiercer-15B-v4 is the latest (v4) iteration in TheDrummer's Snowpiercer series — a fine-tune of Nemotron-15B-Thinker (or similar) architecture, heavily optimized for creative roleplay, immersive storytelling, high creativity, character adherence, and dynamic narrative progression. Users rave about it as 'pretty good', 'feels better than prior versions', 'better than any 12B model I've tried', and 'comparable to last-gen 24Bs' in prose quality, RP steerability, intelligence, reasoning, and forward-moving plot without excessive positivity bias. It knocks out typical alignment slop, enhances RP/creativity (vivid descriptions, unexpected twists, consistent personalities), and retains strong smarts for long sessions. Excellent for SFW epic stories, dark themes, ERP, and TTRPG-style interactions. Community feedback positions it as peak for 15B-and-below class — fast like a 12B (low VRAM needs), but punches way above weight in quality. GGUF quants (from TheDrummer or bartowski) run smoothly on modest hardware; use temperature 0.7–1.0, and follow Drummer's notes for best results. Highly recommended if you want lightweight yet powerful RP model.",
      "description_ru": "Snowpiercer-15B-v4 — это свежая (v4) итерация серии Snowpiercer от TheDrummer: файнтьюн на базе Nemotron-15B-Thinker (или аналогичной), сильно оптимизированный под креативный roleplay, иммерсивный сторителлинг, высокую креативность, приверженность к личностям персонажей и динамичное развитие сюжета. Пользователи в восторге: 'очень хорошо', 'лучше предыдущих версий', 'лучше любого 12B, что пробовал', 'на уровне прошлых 24B' по качеству прозы, управляемость в RP, интеллекту, ризонинг и продвижению истории без избыточной склонности к позитиву. Убирает типичный alignment-slop, усиливает RP/креатив (яркие описания, неожиданные твисты, консистентные личности) и сохраняет сильные мозги для длинных сессий. Отлично работает в SFW-эпике, тёмных темах, ERP и TTRPG-стиле. Отзывы ставят её на пик для 15B и ниже — скорость как у 12B (мало VRAM), но качество далеко впереди. GGUF-кванты (от TheDrummer или bartowski) комфортно летают на среднем железе; используйте температуру 0.7–1.0 и заметки Drummer'а. Топ-рекомендация, если нужен лёгкий, но мощный RP-модель.",
      "author_notes": "Temperature 0.7–1.0 suggested. Focus: creativity, RP steerability, narrative drive, reduced positivity bias. Variants: absolute-heresy (MuXodious). Strong in prose and forward momentum; excels at immersive RP on lighter hardware."
    },
    {
      "hf_id": "SicariusSicariiStuff/Impish_Bloodmoon_12B_GGUF",
      "name": "Impish_Bloodmoon_12B",
      "author": "SicariusSicariiStuff",
      "downloads": 476,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Impish_Bloodmoon_12B is one of the most 'impish' and wicked models in the Impish line — a fine-tune of Mistral-Nemo-Instruct-2407. It's designed for high-heat roleplay, adventure, creative writing, NSFW scenarios, and unaligned tasks without heavy refusals or moralizing. Strengths: very strong theory of mind, reduced positivity bias, vivid prose, high creativity, excellent handling of dark themes, Fallout & Morrowind fandom refinement, multi-language support (Japanese, Hebrew, Russian added), 1-shot JSON roleplay datasets, robust GEC/synonym engine, and powerful creative assistant capabilities. Users call it 'pretty solid', 'feels like Impish Nemo but more wicked', 'fantastic for a 12B', with fun, immersive RP that avoids slop/repetition and pushes boundaries organically. Trained on >1B tokens over ~4 months (merges, CPT, FFT on layers). Community feedback positions it as a top choice for aggressive/hot RP — often preferred over milder siblings like Angelic_Eclipse when you want 'edge' and intensity. Runs comfortably on light hardware (Q4_K_M ~10 GB VRAM). Higher temperature for max creativity.",
      "description_ru": "Impish_Bloodmoon_12B — одна из самых 'озорных' и злых моделей в линейке Impish: файнтьюн Mistral-Nemo-Instruct-2407. Заточена под жаркую ролевую игру, приключения, креативное письмо, NSFW сценарии без сильных отказов или морализаторства. Плюсы: очень сильная понимании личности персонажей, сниженная позитивность, яркая проза, высокая креативность, отличная работа с тёмными темами, поддержка тем Fallout & Morrowind, мультиязычность (Японский, Иврит, Русский добавлены), 1-shot JSON RP-датасеты, надежная система синонимов и мощный креативный ассистент. Пользователи называют 'pretty solid', 'более злая чем Impish Nemo', 'фантастична для 12B' — весёлая, иммерсивная RP без слопа и повторов, и органично раздвигает границы. Обучена на >1B токенов за ~4 месяца (merges, CPT, FFT на слоях). Отзывы ставят её в топ для агрессивного/жаркого RP — предпочитают, когда нужна 'крайность' и интенсивность, в отличие от более мягкой сестры это модели - Angelic_Eclipse. Комфортно на лёгком железе (Q4_K_M ~10 ГБ VRAM). Выше температура для креативности.",
      "author_notes": "Recommended: Roleplay/Adventure format (examples in model card). Settings: Higher temp for creativity. Multi-lang enhanced. Pair with Angelic_Eclipse_12B for contrast (sane/slow-burn). Explicit — NSFW capable."
    },
    {
      "hf_id": "mradermacher/Angelic_Eclipse_12B-i1-GGUF",
      "name": "Angelic_Eclipse_12B",
      "author": "SicariusSicariiStuff",
      "downloads": 500,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Angelic_Eclipse_12B is a fine-tune of Mistral-Nemo-Instruct-2407, designed as the 'sane sister' to the more unhinged Impish_Bloodmoon_12B. It excels at realistic, emotionally intelligent roleplay with exceptional common sense — characters react almost exactly like real people would, without anime tropes or forced escalation unless organically developed. Key strengths: slow-burn progression (gentle, indirect pursuit), strong theory of mind, wholesome yet capable of going 'all the way' when appropriate, vivid prose, high creativity, reduced slop/repetition, and powerful vanilla assistant capabilities. Trained on a massive >1B token dataset (similar to its sibling), it supports multi-language (Japanese, Hebrew, Russian added), 1-shot JSON roleplay formats, and handles adventure/writing/creative tasks coherently. Community feedback praises it for grounded realism, emotional depth, and suitability for immersive, non-rushed RP — often recommended when Impish_Bloodmoon feels too aggressive. Runs comfortably on light hardware (Q4_K_M ~10 GB VRAM). Temperature higher for creativity. Marked as safe-for-most-audiences compared to sibling, but still capable of explicit when prompted organically.",
      "description_ru": "Angelic_Eclipse_12B — это файнтьюн Mistral-Nemo-Instruct-2407, позиционируемый как 'здравая сестра' более дикой Impish_Bloodmoon_12B. Модель блистает в реалистичном, эмоционально умном RP с исключительным чувством здравого смысла — персонажи реагируют почти как реальные люди, без аниме-хаоса или принудительной эскалации, если только это не развивается органично. Ключевые плюсы: мягкое, медленное, косвенное развитие отношений, сильная приверженность личности персонажу, благотворный-тон с возможностью 'идти до конца' когда уместно, яркая проза, высокая креативность, сниженный слоп и повторы и мощный ванильный ассистент. Обучена на огромном датасете >1B токенов (аналогично сестре), поддерживает мультиязычность (добавлены Японский, Иврит, Русский). Отзывы хвалят за реализм, эмоциональную глубину и идеальность для иммерсивного, не-торопливого RP — рекомендуют, когда Impish_Bloodmoon кажется слишком агрессивным. Комфортно на лёгком железе (Q4_K_M ~10 ГБ VRAM). Температура выше для coherent/creativity. Менее чувствительна по сравнению с сестрой, но откровенный RP возможен, если расписан в сюжете органично.",
      "author_notes": "Settings: Higher temp for creativity. Slow-burn realism focus — no rush, strong emotional intelligence. Pair with sibling Impish_Bloodmoon_12B for contrast. Multi-lang support enhanced."
    },
    {
      "hf_id": "SicariusSicariiStuff/Sweet_Dreams_12B_GGUF",
      "name": "Sweet_Dreams_12B",
      "author": "SicariusSicariiStuff",
      "downloads": 320,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
       "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Sweet_Dreams_12B is a versatile 12B fine-tune based on Mistral-Nemo architecture, released in November 2025 and designed as a strong generalist for roleplay and adventure scenarios. The model stands out for its flexibility: it comfortably works with a wide variety of character card and adventure card formats (not limited to the author's SICAtxt style). It features a pleasant, expressive prose style with a unique vocabulary feel, dynamic paragraph lengths (usually 2–3 paragraphs, up to 5), and a clear longer-form bias while remaining responsive and natural. Users particularly appreciate its human-like responses, coherent narrative flow, and ability to handle both classic roleplay and adventure formats effectively. The model includes built-in Morrowind-themed examples and characters, which gives it a recognizable flavor for fantasy/adventure enthusiasts. Censorship is gentle (medium-low, ~6/10 uncensored scale) — rare refusals may appear in assistant mode on extreme topics, but almost none occur in roleplay scenarios. Community feedback highlights the enjoyable writing quality, reduced slop, and overall pleasant vibe. It is frequently described as 'pretty solid', 'very human in responses', and 'surprisingly good at feeling alive'. GGUF quants (especially iMatrix-High-Attention) and FP8 versions run smoothly on mid-range hardware (Q4_K_M ~10 GB VRAM). Temperature 0.7–1.0+ works well for creativity and coherence.",
      "description_ru": "Sweet_Dreams_12B — это универсальный 12B файнтьюн на базе Mistral-Nemo, выпущенный в ноябре 2025 года и созданный как сильный универсал для ролевых игр и приключенческих сценариев. Модель выделяется своей гибкостью: отлично работает с самыми разными форматами карточек персонажей и приключений (не привязана строго к стилю SICAtxt автора). Обладает приятной, выразительной прозой с уникальным ощущением словаря, динамичной длиной параграфов (обычно 2–3, до 5), и заметным уклоном в более длинные ответы, при этом остаётся живой и естественной. Пользователи особенно ценят человекоподобные реакции, связный нарратив и хорошую работу как в классическом roleplay, так и в adventure-форматах. В комплекте идут примеры и персонажи в духе Morrowind, что добавляет модели узнаваемый фэнтезийный оттенок. Цензура мягкая (medium-low, примерно 6/10 по шкале нецензурности) — редкие отказы могут появляться в режиме ассистента на экстремальных темах, но в ролевых сценариях их практически нет. В сообществе модель хвалят за приятное качество письма, низкий уровень слопа и общую 'живую' атмосферу. Часто называют 'pretty solid', 'очень человекоподобная в ответах' и 'приятно удивляет'. GGUF-кванты (особенно iMatrix-High-Attention) и FP8-варианты комфортно работают на среднем железе (Q4_K_M ~10 ГБ VRAM). Температура 0.7–1.0+ хорошо подходит для креативности и связности.",
      "author_notes": "Base: Mistral-Nemo architecture. Longer-form bias with dynamic paragraphs. Censorship: gentle (medium-low). GGUF: iMatrix-High-Attention recommended for best quality. Built-in Morrowind examples and characters. Very flexible with different card formats. Strong prose, human-like feel, high swipe diversity."
    },
    {
      "hf_id": "mradermacher/Famino-12B-Model_Stock-i1-GGUF",
      "name": "Famino-12B-Model_Stock",
      "author": "DreadPoor",
      "downloads": 145,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Famino-12B-Model_Stock is a high-quality 12B merge created using the Model Stock method on base DreadPoor/Ward-12B-Model_Stock, incorporating strong components like cgato/Nemo-12b-Humanize-SFT-v0.2.5-KTO, DreadPoor/Irix-12B-Model_Stock, redrix/GodSlayer-12B-ABYSS, and PygmalionAI/Pygmalion-3-12B. The model is widely regarded as one of the best in the 12B class for writing quality and usability: it delivers coherent, vivid prose, excellent narrative flow, high swipe usability, and strong performance in roleplay, creative writing, and adventure scenarios. Community feedback frequently highlights it as 'the highest ranked 12B in writing category on UGI', 'better than many 70B models in prose', 'really good at writing', and 'slightly edges out Irix in style'. It produces detailed, engaging text with minimal formatting issues or slop, making it especially suitable for immersive storytelling. GGUF quants (especially i1/imatrix from mradermacher) run comfortably on mid-range hardware (Q4_K_M ~10 GB VRAM). Use temperature 0.7–1.0 for optimal creativity and coherence. Often recommended alongside Irix-12B-Model_Stock as a top prose-focused 12B merge.",
      "description_ru": "Famino-12B-Model_Stock — это высококачественный 12B мерж, созданный методом Model Stock на базе DreadPoor/Ward-12B-Model_Stock с включением сильных компонентов: cgato/Nemo-12b-Humanize-SFT-v0.2.5-KTO, DreadPoor/Irix-12B-Model_Stock, redrix/GodSlayer-12B-ABYSS и PygmalionAI/Pygmalion-3-12B. Модель считается одной из лучших в 12B-классе по качеству письма и удобству: выдаёт последовательную, яркую прозу, отличный нарративный поток, высокую полезность свайпов и сильную работу в roleplay, креативности и adventure-сценариях. В сообществе часто называют 'высокооценненной моделью 12B в категории writing на UGI', 'лучше многих 70B по прозе', 'реально хороша в генерации текста' и 'чуть лучше Irix по стилю'. Производит детализированный, увлекательный текст с минимумом проблем форматирования или слопа — идеально для иммерсивного сторителлинга. GGUF-кванты (особенно i1/imatrix от mradermacher) комфортно работают на среднем железе (Q4_K_M ~10 ГБ VRAM). Используйте температуру 0.7–1.0 для лучшей креативности и связности. Часто рекомендуют вместе с Irix-12B-Model_Stock как топовый проза-ориентированный 12B-мердж.",
      "author_notes": "Merge method: Model Stock. Base: DreadPoor/Ward-12B-Model_Stock. Key components: Nemo-Humanize-SFT, Irix-12B, GodSlayer-ABYSS, Pygmalion-3-12B. GGUF quants: mradermacher (i1/imatrix recommended for highest quality). Temperature 0.7–1.0 suggested. Focus: exceptional prose, high swipe usability, coherent writing. Strong in roleplay and creative scenarios. Often compared favorably to Irix with slight edge in writing quality."
    },
    {
      "hf_id": "DreadPoor/Irix-12B-Model_Stock-GGUF",
      "name": "Irix-12B-Model_Stock",
      "author": "DreadPoor",
      "downloads": 108,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Irix-12B-Model_Stock is a high-quality 12B merge created using the Model Stock method (arxiv:2403.19522) on base yamatazen/EtherealAurora-12B-v2, incorporating strengths from Faber-12-Model_Stock, Violet-Lyra-Gutenberg-v2, patricide-12B-Unslop-Mell-v2, and EtherealAurora-12B-v3. It delivers strong prose, creative and coherent storytelling, excellent writing quality, and consistent performance in roleplay scenarios with minimal formatting issues. Users frequently praise it as one of the best 12B models for immersive narratives: 'smart and consistent', 'almost every swipe is usable', 'highest ranked in writing category', 'best model I've tested so far' — with vivid descriptions, good reasoning, and reliable adherence to prompts without heavy repetition or slop. It handles long sessions well, producing detailed, engaging text suitable for adventures, creative writing, and character-driven interactions. Feedback positions it as a top lightweight contender (punches above weight for 12B), often compared favorably to other Nemo/Mistral merges in prose and usability. GGUF quants (especially i1/imatrix from mradermacher) run smoothly on modest hardware (Q4_K_M ~10 GB VRAM); use standard chat templates and moderate temperature (0.7–1.0) for optimal creativity and coherence.",
      "description_ru": "Irix-12B-Model_Stock — это высококачественный 12B мерж, созданный методом Model Stock (arxiv:2403.19522) на базе yamatazen/EtherealAurora-12B-v2 с интеграцией Faber-12-Model_Stock, Violet-Lyra-Gutenberg-v2, patricide-12B-Unslop-Mell-v2 и EtherealAurora-12B-v3. Модель выдаёт сильную прозу, креативный и последовательный сторителлинг, отличное качество письма и последовательную работу в ролевых сценариях с минимумом проблем форматирования. Пользователи часто называют её одной из лучших 12B для иммерсивных нарративов: 'умный и последовательный', 'почти каждый свайп можно использовать', 'высокая позиция в категории написания текста.', 'лучшая модель, которую я тестировал' — с яркими описаниями, хорошим ризонингом и надёжным следованием промптам без сильных повторений или слопа. Хорошо держит длинные сессии, производя детализированный, увлекательный текст для приключений, креативного письма и персонаже-ориентированные диалоги. По отзывам — топовый лёгкий кандидат (бьёт выше своего веса для 12B), часто сравнивают с другими Nemo/Mistral-мерджами по прозе и удобству. GGUF-кванты (особенно i1/imatrix от mradermacher) комфортно летают на среднем железе (Q4_K_M ~10 ГБ VRAM); используйте умеренную температуру (0.7–1.0) для лучшей креативности.",
      "author_notes": "Merge method: Model Stock. Base: yamatazen/EtherealAurora-12B-v2. Components: Faber-12-Model_Stock, Violet-Lyra-Gutenberg-v2, patricide-12B-Unslop-Mell-v2, EtherealAurora-12B-v3. GGUF quants: mradermacher (i1/imatrix recommended for quality). Temperature 0.7–1.0 suggested. Strong in prose/writing. 32K context capable in some setups."
    },
    {
      "hf_id": "ReadyArt/Dark-Desires-12B-v1.5-GGUF",
      "name": "Dark-Desires-12B-v1.5",
      "author": "ReadyArt",
      "downloads": 1290,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 10,
      "optimal_quant": "Q4_K_M",
      "description_en": "Dark-Desires-12B-v1.5 (and v1.0 series) is a highly uncensored, explicit-focused 12B model from ReadyArt, designed for dangerous ERP, conversational roleplay, NSFW scenarios, and 'down-to-earth' dark themes without refusals or moralizing. It's praised for excellent instruction following (even in custom formats), high coherence, natural and grounded responses, strong escalation in spicy content, and suitability for immersive roleplaying systems. Users describe it as 'extremely good' at maintaining vibe, avoiding slop/repetition, and feeling 'made especially for roleplaying' — with vivid, explicit prose that handles dark or intense interactions confidently. Part of ReadyArt's 'dark' lineup (alongside Darkness-Incarnate), it's a go-to for users seeking truly unfiltered NSFW/ERP on lighter hardware. Early feedback highlights it as one of the stronger 12B options for explicit RP, often compared favorably to Nemo-based merges or smaller uncensored tunes. Runs comfortably on modest setups (Q4_K_M ~10 GB VRAM); use temperature 0.7–1.0 and be mindful of sensitive content warnings — this is explicitly NSFW.",
      "description_ru": "Dark-Desires-12B-v1.5 (и серия v1.0) — это нецензурная 12B-модель от ReadyArt, созданная для опасного ERP, разговорного ролевого опыта, NSFW-сценариев и 'приземлённого' тёмного контента без отказов или морализаторства. Пользователи хвалят отличное следование инструкциям (даже в кастомных форматах), естественные и приземлённые ответы, идеальную подгонку под иммерсивные ролевые системы. Описывают как 'экстремально хорошую' в поддержании атмосферы, избежании повторов и ощущении того, что модель сделана спецаильное под ролевые игры — с яркой прозой, уверенно справляющейся с тёмными или интенсивными взаимодействиями. Часть линейки ReadyArt 'dark' (вместе с Darkness-Incarnate), это один из топовых 12B-вариантов для тёмных RP. Отзывы называют её сильным игроком среди нецензурных моделей 12B, часто сравнивают с Nemo-мерджами. Комфортно запускается на скромном железе (Q4_K_M ~10 ГБ VRAM); температура 0.7–1.0, будьте готовы к чувствительному-контенту — модель явно NSFW.",
      "author_notes": "Base likely Mistral-Nemo or similar (explicit fine-tune/merge). GGUF quants from ReadyArt recommended (Q4_K_M/Q5_K_M for balance). Marked as sensitive/NSFW — contains explicit, dangerous content. Use with caution; excels at ERP/dark RP. Temperature 0.7–1.0 suggested for natural flow. Part of ReadyArt collection with variants (22B, 32B Dark-Desires)."
    },
    {
      "hf_id": "IlyaGusev/saiga_gemma3_12b_gguf",
      "name": "Saiga Gemma3 12B GGUF",
      "author": "IlyaGusev",
      "downloads": 850,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 8,
      "optimal_quant": "Q5_K_M",
      "description_en": "Saiga Gemma3 12B GGUF is a highly regarded Russian-language fine-tune of Google Gemma-3-12B-it (abliterated version by mlabonne for reduced censorship), released in April 2025. It is one of the strongest open-source models for high-quality Russian in the 12B class: excellent grammar, natural phrasing, minimal spelling/orthography errors, strong instruction following, coherent long dialogues, and good performance in roleplay, adventure, and creative writing scenarios. The model supports uncensored output in most cases (rare refusals only on extreme topics), handles 128K context, and excels at emotionally nuanced, realistic Russian conversations. Community feedback frequently calls it 'лучшая русскоязычная модель в 12B-классе', 'отлично держит русский без косяков', 'показала себя достойно в ролевых сценариях', 'лучше любых мультиязычных на русском'. It is widely used in SillyTavern, KoboldCPP, and Ollama for Russian RP, chat, and storytelling. GGUF quants (especially Q5_K_M and Q6_K) run smoothly on mid-range hardware (Q5 ~8–10 GB VRAM). Recommended for users needing native-level Russian without heavy hardware requirements. Temperature 0.7–1.0 works well.",
      "description_ru": "Saiga Gemma3 12B GGUF — это высокооценённый русскоязычный файнтьюн Google Gemma-3-12B-it (abliterated-версия от mlabonne для снижения цензуры), выпущенный в апреле 2025 года. Одна из самых сильных open-source моделей на русском языке в 12B-классе: отличная грамматика, естественная речь, минимум орфографических/стилистических ошибок, сильное следование инструкциям, последовательные длинные диалоги и хорошая работа в roleplay, adventure и креативном письме. Поддерживает NSFW вывод (редкие отказы только на экстремальных темах), контекст 128K и эмоционально нюансированные, реалистичные русскоязычные разговоры. В сообществе часто называют 'лучшая русскоязычная модель в 12B', 'отлично держит русский без косяков', 'достойно показала себя в ролевых сценариях', 'лучше любых мультиязычных на русском'. Широко используется в SillyTavern, KoboldCPP и Ollama для русского RP, чата и сторителлинга. GGUF-кванты (особенно Q5_K_M и Q6_K) комфортно работают на среднем железе (Q5 ~8–10 ГБ VRAM). Рекомендуется для тех, кому нужен нативный русский без тяжёлого оборудования. Температура 0.7–1.0 хорошо подходит.",
      "author_notes": "Base: google/gemma-3-12b-it (abliterated by mlabonne). Format: ChatML. Context: 128K. Censorship: low-medium (abliterated). GGUF quants: IlyaGusev/saiga_gemma3_12b_gguf (Q4_K_M, Q5_K_M, Q6_K recommended). Optimal: Q5_K_M or Q6_K for best Russian quality. Strong in native Russian RP, instruction following, coherent long-form responses. Very popular in Russian-speaking AI communities."
    },
    {
      "hf_id": "temaq-org/Tema_Q-R3.1",
      "name": "Tema_Q-R3.1 9B",
      "author": "temaq-org",
      "downloads": 280,
      "min_ram_gb": 8,
      "recommended_ram_gb": 12,
      "min_vram_gb": 0,
      "recommended_vram_gb": 8,
      "optimal_quant": "Q5_K_M",
      "description_en": "Tema_Q-R3.1 is a high-NatInt (Natural Intelligence) uncensored 9B model based on Google Gemma-2-9B, released as the 'Excellent First Release' in the Tema_Q-R series in late 2025–early 2026. It is designed as an indie flagship for Japanese, English, and Chinese languages, boasting one of the highest uncensored intelligence scores among independent LLMs. The model excels at natural, coherent conversations, creative writing, roleplay, and general tasks with minimal refusals or corporate-style censorship. Users in Japanese/Chinese communities and uncensored LLM collections praise it for 'extremely natural Japanese', 'strong uncensored performance', 'high NatInt', and 'one of the best Gemma-2 9B variants'. It handles multilingual RP and storytelling well, with vivid prose and good instruction following. GGUF quants (especially i1/imatrix from mradermacher) run smoothly on mid-range hardware (Q5_K_M ~8 GB VRAM). Recommended for users seeking powerful uncensored Asian-language support or indie high-intelligence models. Temperature 0.7–1.0 works well. Part of the larger Tema_Q-R lineup (R6.0/R7.0 12B variants available).",
      "description_ru": "Tema_Q-R3.1 — это высокоинтеллектуальная uncensored 9B-модель на базе Google Gemma-2-9B, выпущенная как 'Excellent First Release' в серии Tema_Q-R в конце 2025 – начале 2026 года. Позиционируется как инди-флагман с одним из самых высоких показателей NatInt (Natural Intelligence) среди независимых LLM, с основным фокусом на японский, английский и китайский языки. Модель отлично справляется с естественными, последовательными разговорами, креативным письмом, roleplay и общими задачами с минимальными отказами и корпоративной цензурой. В японском/китайском сообществе и коллекциях uncensored LLM её хвалят за 'чрезвычайно естественный японский', 'сильную uncensored производительность', 'высокий NatInt' и 'одну из лучших вариаций Gemma-2 9B'. Хорошо работает в мультиязычном RP и сторителлинге, с яркой прозой и надёжным следованием инструкциям. GGUF-кванты (особенно i1/imatrix от mradermacher) комфортно запускаются на среднем железе (Q5_K_M ~8 ГБ VRAM). Рекомендуется для тех, кто ищет мощную uncensored модель с поддержкой азиатских языков или инди-LLM с высоким интеллектом. Температура 0.7–1.0 хорошо подходит. Часть большой линейки Tema_Q-R (есть R6.0/R7.0 12B-варианты).",
      "author_notes": "Base: google/gemma-2-9b. Focus: Japanese/English/Chinese languages. Uncensored with high NatInt (Natural Intelligence score). GGUF quants: mradermacher (i1/imatrix recommended for best quality). Temperature 0.7–1.0 suggested. Strong in natural multilingual conversations, creative writing, and roleplay. Minimal censorship. Part of Tema_Q-R series (flagship R3.1 9B, larger R6.0/R7.0 12B available). Excellent first release in the lineup."
    },
    {
      "hf_id": "SicariusSicariiStuff/Impish_LLAMA_4B_GGUF",
      "name": "Impish_LLAMA_4B",
      "author": "SicariusSicariiStuff",
      "downloads": 500,
      "min_ram_gb": 6,
      "recommended_ram_gb": 8,
      "min_vram_gb": 0,
      "recommended_vram_gb": 6,
      "optimal_quant": "Q4_K_M",
      "description_en": "Impish_LLAMA_4B is a highly capable 4B fine-tune of NVIDIA's Llama-3.1-Minitron-4B-Width-Base, optimized for roleplay, adventure, creative writing, and general tasks with surprising intelligence and 'soul' for its size. Trained on over 400M tokens (plus 200M in July 2025 retrain for ChatML fixes and improvements), it features reduced positivity bias (Negative_LLAMA-style data adjusted for 4B), extended 4chan dataset for humanity/quirkiness/argumentativeness, combat scenarios, strong character agency (they can surprise you), excellent context handling (tested up to 16K+), and responsive 'in the moment' feel. Users praise it as 'incredibly powerful', 'kicks far above its weight', 'one of the best tiny roleplay models ever', with vivid prose, coherent narratives, minimal slop, and decent assistant capabilities. It runs extremely fast/lightweight (even on Raspberry Pi 5 or mid-tier phones), supports mostly uncensored output while retaining smarts, and excels at short-to-medium responses (1-3 paragraphs, CAI-style). Community feedback highlights context awareness, agency, and adventure quality — often called a 'peak' for <5B class. Use temperature 0.7–1.0+ for creativity. Minor issues like repetition/hallucinations fixable with good prompting.",
      "description_ru": "Impish_LLAMA_4B — это очень мощный 4B файнтьюн на базе NVIDIA Llama-3.1-Minitron-4B-Width-Base, заточенный под roleplay, приключения, креативное письмо и общие задачи с удивительной 'душой' и интеллектом для своего размера. Обучена на >400M токенов (плюс 200M в ретрейне июля 2025 для фикса ChatML и улучшений), с сниженной позитивностью (Negative_LLAMA-стиль, адаптированный под 4B), расширенным 4chan-датасетом для человечности, изворотливости, умения спорить, боевыми сценариями, сильной организацией персонажей (могут удивлять), отличным удержанием контекста (тестировано до 16K+). Пользователи называют 'исключительно мощный', 'уходит в качестве далеко за свой размер', 'одна из лучших маленьких моделей для ролевой игры' — яркая проза, последовательные нарративы, минимум слопа и достойные возможности ассистента. Летает сверхбыстро/лёгко (даже на Raspberry Pi 5 или средних телефонах), поддерживает NSFW вывод при сохранении ума, идеально для коротких-средних ответов (1-3 параграфа, CAI-стиль). Отзывы подчёркивают знание контекста, организованность и качество приключений — часто называют 'пиком' для <5B-класса. Используйте температуру 0.7–1.0+ для креативности. Мелкие проблемы вроде повторов и галлюцинаций фиксятся хорошими промптами.",
      "author_notes": "Base: nvidia/Llama-3.1-Minitron-4B-Width-Base. Format: ChatML (retrained July 2025). Recommended: Roleplay/Adventure format (examples in model card). Settings: Temperature 0.7–1.0+, short-medium responses. Focus: agency, reduced positivity, quirkiness from 4chan data, combat/refined adventure. Runs on very low hardware — Raspberry Pi 5 capable."
    }
  ]
}
